{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619283e8",
   "metadata": {},
   "source": [
    "# CNN Web Scraping consolidated script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dbb16d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jupyter notebook --NotebookApp.iopub_data_rate_limit=1.0e10\n",
    "# ^ make sure this can actually parse relevant data\n",
    "import urllib.request, sys, time\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import openpyxl\n",
    "import urllib.request as client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe6c6e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(URL):\n",
    "    '''\n",
    "    Takes in URL of a CNN webpage (str) and returns title of article (str) and body text of article (str).\n",
    "    '''\n",
    "    page = requests.get(URL)\n",
    "    if page.status_code == 200: # if this request is successful - 200 message demarcates this\n",
    "        soup = BeautifulSoup(page.text, \"html.parser\")    \n",
    "\n",
    "        # finding title of article\n",
    "        title = str(soup.find_all('title'))\n",
    "        title = str(title).split('[<title>')[1].split(' |')[0]\n",
    "\n",
    "        # find text of article\n",
    "        scripts = soup.find_all(\"script\", string=re.compile(\"@context\"))\n",
    "        l = list(scripts)\n",
    "        l[0] = str(l[0])\n",
    "        l1 = l[0].split('\"articleBody\":\"')\n",
    "        l2 = l1[1].split('\",\"articleSection\"')\n",
    "        string = l2[0].replace('\\xa0', ' ')\n",
    "    \n",
    "        return title, string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5ea0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_base_urls(base_url, add_ons):\n",
    "    \"\"\"\n",
    "    Takes in base url for CNN (str) and generates list of base URLs (str) to subsequently parse.\n",
    "    \"\"\"\n",
    "    return [base_url + item for item in add_ons]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea233952",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dates(url, start_year, end_year):\n",
    "    \"\"\"\n",
    "    Takes in base url (str), start year (int) and end year (int) and generates a list of URLs \n",
    "    marked by month and year to parse through. \n",
    "    \"\"\"\n",
    "    # help from https://medium.com/swlh/web-scraping-with-less-than-20-lines-of-code-b363c9e0153a\n",
    "    art = []\n",
    "    dates = []\n",
    "\n",
    "    # generate strings of dates\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        for month in range(1, 13):\n",
    "            date = str(year) + '-' + str(month) + '.html'\n",
    "            dates.append(date)\n",
    "            \n",
    "    #base url for CNN\n",
    "    base = url + '/article/sitemap-'\n",
    "\n",
    "    # add relevant URLS\n",
    "    for date in dates:\n",
    "        url = base + date\n",
    "        art.append(url)\n",
    "    return art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "766770ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_urls(dated_urls, subject):\n",
    "    \"\"\"\n",
    "    Takes in list of dated urls to do with a specific subject matter (e.g. politics, business) and returns\n",
    "    list of relevant URLs of articles to parse. \n",
    "    \"\"\"\n",
    "    all_pol = []\n",
    "\n",
    "    for html in dated_urls:\n",
    "        soup = BeautifulSoup(client.urlopen(html), 'html.parser')\n",
    "        for link in soup.find_all('a'):\n",
    "            all_pol.append(link.get('href'))\n",
    "            \n",
    "    # get rid of NaN values\n",
    "    all_pol = list(filter(None,all_pol))\n",
    "    \n",
    "    subject = '/' + subject + '/'\n",
    "    \n",
    "    # ensure that articles are relevant to subject matter and are dated\n",
    "    all_pol_final = [link for link in all_pol if link.startswith('https://www.cnn.com/20') and subject in link]\n",
    "    \n",
    "    return all_pol_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18831ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_file(url_list, subject):\n",
    "    \"\"\"\n",
    "    Takes in list of URLs and subject name (str) and creates a line-separated text file to store URL \n",
    "    information. \n",
    "    \"\"\"\n",
    "    # generate file name\n",
    "    # help from https://www.geeksforgeeks.org/read-a-file-line-by-line-in-python/\n",
    "    name = 'cnn_' + subject + '_urls.txt'\n",
    "    file = open(name,'w') # remember w command overwrites any file with existing name\n",
    "    for item in url_list:\n",
    "        file.write(item+\"\\n\")\n",
    "    file.close()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de69ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df(url_file):\n",
    "    '''\n",
    "    Parse through generated text file of URLs and return pandas dataframe of titles and body text.\n",
    "    '''\n",
    "    # this will take a LONG time to run, so do so in command line with .py file in virtual environment\n",
    "    # initialise lists for data\n",
    "    titles = []\n",
    "    body_text = []\n",
    "    \n",
    "    file = open(url_file, 'r') # open input file\n",
    "    lst = []\n",
    "    \n",
    "    # create list with all URLs, replacing line breaks\n",
    "    while True:\n",
    "        line = file.readline()\n",
    "        if not line:\n",
    "            break\n",
    "        line = line.replace('\\n', '')\n",
    "        lst.append(line)\n",
    "        \n",
    "    # identify titles and body texts of the \n",
    "    for i, URL in enumerate(lst):\n",
    "        print(i)\n",
    "        URL = URL.replace('\\n', '')\n",
    "        title, text = parse(URL)\n",
    "        print(title)\n",
    "        titles.append(title)\n",
    "        body_text.append(text)\n",
    "    \n",
    "    titles = np.array(titles)\n",
    "    body_text = np.array(body_text)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Title': titles,\n",
    "        'Body_Text': body_text\n",
    "    })\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e456755c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cnn.com/us',\n",
       " 'https://www.cnn.com/world',\n",
       " 'https://www.cnn.com/politics',\n",
       " 'https://www.cnn.com/business',\n",
       " 'https://www.cnn.com/opinions']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://www.cnn.com/'\n",
    "add_ons = ['us', 'world', 'politics', 'business', 'opinions']\n",
    "\n",
    "base_urls = create_base_urls(base_url, add_ons)\n",
    "base_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68b76d9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cnn.com/us/article/sitemap-2008-1.html',\n",
       " 'https://www.cnn.com/us/article/sitemap-2008-2.html',\n",
       " 'https://www.cnn.com/us/article/sitemap-2008-3.html',\n",
       " 'https://www.cnn.com/us/article/sitemap-2008-4.html',\n",
       " 'https://www.cnn.com/us/article/sitemap-2008-5.html']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dated_us = generate_dates(base_urls[0], 2008, 2023)\n",
    "dated_us[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f465ca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.cnn.com/2011/05/02/us/new-levee-breach/index.html',\n",
       " 'https://www.cnn.com/2011/05/31/us/missouri-joplin-death-toll/index.html',\n",
       " 'https://www.cnn.com/2011/07/20/us/heat/index.html',\n",
       " 'https://www.cnn.com/2011/07/11/us/us-new-mexico-fire-flooding/index.html',\n",
       " 'https://www.cnn.com/2011/07/11/us/us-space-shuttle/index.html']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "us_urls = generate_urls(dated_us, 'us')\n",
    "us_urls[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cd8519a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56717"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(us_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d80cea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file(us_urls, 'us')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
